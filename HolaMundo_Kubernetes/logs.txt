* 
* ==> Audit <==
* |------------|--------------------|----------|--------------------------------|---------|---------------------|---------------------|
|  Command   |        Args        | Profile  |              User              | Version |     Start Time      |      End Time       |
|------------|--------------------|----------|--------------------------------|---------|---------------------|---------------------|
| start      |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 19:15 -05 |                     |
|            |                    |          | Encalada                       |         |                     |                     |
| start      |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 19:27 -05 |                     |
|            |                    |          | Encalada                       |         |                     |                     |
| start      |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 19:36 -05 | 04 Aug 23 19:40 -05 |
|            |                    |          | Encalada                       |         |                     |                     |
| start      |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:08 -05 | 04 Aug 23 20:09 -05 |
|            |                    |          | Encalada                       |         |                     |                     |
| service    | hola-mundo-service | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:23 -05 |                     |
|            |                    |          | Encalada                       |         |                     |                     |
| service    | hola-mundo-service | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:24 -05 |                     |
|            |                    |          | Encalada                       |         |                     |                     |
| start      |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:28 -05 | 04 Aug 23 20:28 -05 |
|            |                    |          | Encalada                       |         |                     |                     |
| docker-env |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:28 -05 | 04 Aug 23 20:28 -05 |
|            |                    |          | Encalada                       |         |                     |                     |
| start      |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:29 -05 | 04 Aug 23 20:29 -05 |
|            |                    |          | Encalada                       |         |                     |                     |
| docker-env |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:29 -05 | 04 Aug 23 20:29 -05 |
|            |                    |          | Encalada                       |         |                     |                     |
| service    | hola-mundo-service | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:31 -05 |                     |
|            |                    |          | Encalada                       |         |                     |                     |
| stop       |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:32 -05 | 04 Aug 23 20:32 -05 |
|            |                    |          | Encalada                       |         |                     |                     |
| start      |                    | minikube | DESKTOP-A6UFJ99\Christian      | v1.31.1 | 04 Aug 23 20:33 -05 | 04 Aug 23 20:34 -05 |
|            |                    |          | Encalada                       |         |                     |                     |
|------------|--------------------|----------|--------------------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/08/04 20:33:28
Running on machine: DESKTOP-A6UFJ99
Binary: Built with gc go1.20.6 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0804 20:33:28.368961   16908 out.go:296] Setting OutFile to fd 88 ...
I0804 20:33:28.395260   16908 out.go:348] isatty.IsTerminal(88) = true
I0804 20:33:28.395260   16908 out.go:309] Setting ErrFile to fd 92...
I0804 20:33:28.395260   16908 out.go:348] isatty.IsTerminal(92) = true
I0804 20:33:28.395815   16908 oci.go:573] shell is pointing to dockerd inside minikube. will unset to use host
W0804 20:33:28.408908   16908 root.go:314] Error reading config file at C:\Users\Christian Encalada\.minikube\config\config.json: open C:\Users\Christian Encalada\.minikube\config\config.json: The system cannot find the file specified.
I0804 20:33:28.413217   16908 out.go:303] Setting JSON to false
I0804 20:33:28.418461   16908 start.go:128] hostinfo: {"hostname":"DESKTOP-A6UFJ99","uptime":3568,"bootTime":1691195640,"procs":315,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.3208 Build 19045.3208","kernelVersion":"10.0.19045.3208 Build 19045.3208","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"e407b1f2-007a-44cd-a436-9f3dec945a49"}
W0804 20:33:28.418461   16908 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0804 20:33:28.421086   16908 out.go:177] 😄  minikube v1.31.1 on Microsoft Windows 10 Pro 10.0.19045.3208 Build 19045.3208
I0804 20:33:28.422675   16908 notify.go:220] Checking for updates...
I0804 20:33:28.423757   16908 out.go:177]     ▪ MINIKUBE_ACTIVE_DOCKERD=minikube
I0804 20:33:28.425284   16908 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0804 20:33:28.425284   16908 driver.go:373] Setting default libvirt URI to qemu:///system
I0804 20:33:28.584377   16908 docker.go:121] docker version: linux-24.0.5:Docker Desktop 4.22.0 (117440)
I0804 20:33:28.593309   16908 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0804 20:33:28.862099   16908 info.go:266] docker info: {ID:e38f15e5-a7ae-4401-bec2-8c800649cf4b Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:true NGoroutines:93 SystemTime:2023-08-05 01:33:28.816752366 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:13343838208 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0804 20:33:28.865841   16908 out.go:177] ✨  Using the docker driver based on existing profile
I0804 20:33:28.866914   16908 start.go:298] selected driver: docker
I0804 20:33:28.866914   16908 start.go:898] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Christian Encalada:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0804 20:33:28.866914   16908 start.go:909] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0804 20:33:28.889967   16908 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0804 20:33:29.168706   16908 info.go:266] docker info: {ID:e38f15e5-a7ae-4401-bec2-8c800649cf4b Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:63 OomKillDisable:true NGoroutines:93 SystemTime:2023-08-05 01:33:29.123298504 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:13343838208 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.20.2-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.6] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:0.20.0]] Warnings:<nil>}}
I0804 20:33:29.199479   16908 cni.go:84] Creating CNI manager for ""
I0804 20:33:29.199479   16908 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0804 20:33:29.199479   16908 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Christian Encalada:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0804 20:33:29.201060   16908 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0804 20:33:29.202654   16908 cache.go:122] Beginning downloading kic base image for docker with docker
I0804 20:33:29.203703   16908 out.go:177] 🚜  Pulling base image ...
I0804 20:33:29.205249   16908 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0804 20:33:29.205249   16908 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0804 20:33:29.205306   16908 preload.go:148] Found local preload: C:\Users\Christian Encalada\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4
I0804 20:33:29.205306   16908 cache.go:57] Caching tarball of preloaded images
I0804 20:33:29.205306   16908 preload.go:174] Found C:\Users\Christian Encalada\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.27.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0804 20:33:29.205306   16908 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.3 on docker
I0804 20:33:29.205832   16908 profile.go:148] Saving config to C:\Users\Christian Encalada\.minikube\profiles\minikube\config.json ...
I0804 20:33:29.338120   16908 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I0804 20:33:29.338120   16908 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I0804 20:33:29.338120   16908 cache.go:195] Successfully downloaded all kic artifacts
I0804 20:33:29.338120   16908 start.go:365] acquiring machines lock for minikube: {Name:mk7b38fb28bdc377f5fcdf0e579e6a89b58b8eb4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0804 20:33:29.338120   16908 start.go:369] acquired machines lock for "minikube" in 0s
I0804 20:33:29.338655   16908 start.go:96] Skipping create...Using existing machine configuration
I0804 20:33:29.338655   16908 fix.go:54] fixHost starting: 
I0804 20:33:29.340267   16908 out.go:177] 📌  Noticed you have an activated docker-env on docker driver in this terminal:
W0804 20:33:29.341336   16908 out.go:239] ❗  Please re-eval your docker-env, To ensure your environment variables have updated ports:

	'minikube -p minikube docker-env'

	
I0804 20:33:29.359884   16908 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0804 20:33:29.490421   16908 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0804 20:33:29.490421   16908 fix.go:128] unexpected machine state, will restart: <nil>
I0804 20:33:29.491466   16908 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0804 20:33:29.501959   16908 cli_runner.go:164] Run: docker start minikube
I0804 20:33:31.249157   16908 cli_runner.go:217] Completed: docker start minikube: (1.7470693s)
I0804 20:33:31.260324   16908 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0804 20:33:31.387658   16908 kic.go:426] container "minikube" state is running.
I0804 20:33:31.398315   16908 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0804 20:33:31.510716   16908 profile.go:148] Saving config to C:\Users\Christian Encalada\.minikube\profiles\minikube\config.json ...
I0804 20:33:31.511787   16908 machine.go:88] provisioning docker machine ...
I0804 20:33:31.511787   16908 ubuntu.go:169] provisioning hostname "minikube"
I0804 20:33:31.521526   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:31.652278   16908 main.go:141] libmachine: Using SSH client type: native
I0804 20:33:31.652789   16908 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1108da0] 0x110bc40 <nil>  [] 0s} 127.0.0.1 53726 <nil> <nil>}
I0804 20:33:31.652852   16908 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0804 20:33:31.896835   16908 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0804 20:33:31.906470   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:32.036800   16908 main.go:141] libmachine: Using SSH client type: native
I0804 20:33:32.036800   16908 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1108da0] 0x110bc40 <nil>  [] 0s} 127.0.0.1 53726 <nil> <nil>}
I0804 20:33:32.036800   16908 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0804 20:33:32.114890   16908 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0804 20:33:32.114890   16908 ubuntu.go:175] set auth options {CertDir:C:\Users\Christian Encalada\.minikube CaCertPath:C:\Users\Christian Encalada\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Christian Encalada\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Christian Encalada\.minikube\machines\server.pem ServerKeyPath:C:\Users\Christian Encalada\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Christian Encalada\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Christian Encalada\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Christian Encalada\.minikube}
I0804 20:33:32.114890   16908 ubuntu.go:177] setting up certificates
I0804 20:33:32.114890   16908 provision.go:83] configureAuth start
I0804 20:33:32.124410   16908 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0804 20:33:32.248708   16908 provision.go:138] copyHostCerts
I0804 20:33:32.248708   16908 exec_runner.go:144] found C:\Users\Christian Encalada\.minikube/ca.pem, removing ...
I0804 20:33:32.248708   16908 exec_runner.go:203] rm: C:\Users\Christian Encalada\.minikube\ca.pem
I0804 20:33:32.249252   16908 exec_runner.go:151] cp: C:\Users\Christian Encalada\.minikube\certs\ca.pem --> C:\Users\Christian Encalada\.minikube/ca.pem (1111 bytes)
I0804 20:33:32.249787   16908 exec_runner.go:144] found C:\Users\Christian Encalada\.minikube/cert.pem, removing ...
I0804 20:33:32.249787   16908 exec_runner.go:203] rm: C:\Users\Christian Encalada\.minikube\cert.pem
I0804 20:33:32.249787   16908 exec_runner.go:151] cp: C:\Users\Christian Encalada\.minikube\certs\cert.pem --> C:\Users\Christian Encalada\.minikube/cert.pem (1155 bytes)
I0804 20:33:32.250317   16908 exec_runner.go:144] found C:\Users\Christian Encalada\.minikube/key.pem, removing ...
I0804 20:33:32.250317   16908 exec_runner.go:203] rm: C:\Users\Christian Encalada\.minikube\key.pem
I0804 20:33:32.250317   16908 exec_runner.go:151] cp: C:\Users\Christian Encalada\.minikube\certs\key.pem --> C:\Users\Christian Encalada\.minikube/key.pem (1675 bytes)
I0804 20:33:32.250854   16908 provision.go:112] generating server cert: C:\Users\Christian Encalada\.minikube\machines\server.pem ca-key=C:\Users\Christian Encalada\.minikube\certs\ca.pem private-key=C:\Users\Christian Encalada\.minikube\certs\ca-key.pem org=Christian Encalada.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0804 20:33:32.345283   16908 provision.go:172] copyRemoteCerts
I0804 20:33:32.359283   16908 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0804 20:33:32.368417   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:32.481290   16908 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53726 SSHKeyPath:C:\Users\Christian Encalada\.minikube\machines\minikube\id_rsa Username:docker}
I0804 20:33:32.570092   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1111 bytes)
I0804 20:33:32.589986   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\machines\server.pem --> /etc/docker/server.pem (1233 bytes)
I0804 20:33:32.610020   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0804 20:33:32.629081   16908 provision.go:86] duration metric: configureAuth took 514.1909ms
I0804 20:33:32.629081   16908 ubuntu.go:193] setting minikube options for container-runtime
I0804 20:33:32.629081   16908 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0804 20:33:32.638070   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:32.761326   16908 main.go:141] libmachine: Using SSH client type: native
I0804 20:33:32.761845   16908 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1108da0] 0x110bc40 <nil>  [] 0s} 127.0.0.1 53726 <nil> <nil>}
I0804 20:33:32.761845   16908 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0804 20:33:32.840998   16908 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0804 20:33:32.840998   16908 ubuntu.go:71] root file system type: overlay
I0804 20:33:32.840998   16908 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0804 20:33:32.849960   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:32.977408   16908 main.go:141] libmachine: Using SSH client type: native
I0804 20:33:32.977922   16908 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1108da0] 0x110bc40 <nil>  [] 0s} 127.0.0.1 53726 <nil> <nil>}
I0804 20:33:32.977922   16908 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0804 20:33:33.123705   16908 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0804 20:33:33.132745   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:33.256018   16908 main.go:141] libmachine: Using SSH client type: native
I0804 20:33:33.256524   16908 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1108da0] 0x110bc40 <nil>  [] 0s} 127.0.0.1 53726 <nil> <nil>}
I0804 20:33:33.256524   16908 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0804 20:33:33.389439   16908 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0804 20:33:33.389439   16908 machine.go:91] provisioned docker machine in 1.8776521s
I0804 20:33:33.389439   16908 start.go:300] post-start starting for "minikube" (driver="docker")
I0804 20:33:33.389439   16908 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0804 20:33:33.403171   16908 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0804 20:33:33.411680   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:33.532579   16908 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53726 SSHKeyPath:C:\Users\Christian Encalada\.minikube\machines\minikube\id_rsa Username:docker}
I0804 20:33:33.593862   16908 ssh_runner.go:195] Run: cat /etc/os-release
I0804 20:33:33.597807   16908 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0804 20:33:33.597807   16908 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0804 20:33:33.597807   16908 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0804 20:33:33.597807   16908 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0804 20:33:33.597807   16908 filesync.go:126] Scanning C:\Users\Christian Encalada\.minikube\addons for local assets ...
I0804 20:33:33.597807   16908 filesync.go:126] Scanning C:\Users\Christian Encalada\.minikube\files for local assets ...
I0804 20:33:33.597807   16908 start.go:303] post-start completed in 208.3687ms
I0804 20:33:33.611680   16908 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0804 20:33:33.620109   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:33.731622   16908 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53726 SSHKeyPath:C:\Users\Christian Encalada\.minikube\machines\minikube\id_rsa Username:docker}
I0804 20:33:33.789390   16908 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0804 20:33:33.793934   16908 fix.go:56] fixHost completed within 4.4552784s
I0804 20:33:33.793934   16908 start.go:83] releasing machines lock for "minikube", held for 4.4558137s
I0804 20:33:33.802973   16908 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0804 20:33:33.918473   16908 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0804 20:33:33.928560   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:33.934006   16908 ssh_runner.go:195] Run: cat /version.json
I0804 20:33:33.943727   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:33:34.054557   16908 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53726 SSHKeyPath:C:\Users\Christian Encalada\.minikube\machines\minikube\id_rsa Username:docker}
I0804 20:33:34.064108   16908 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53726 SSHKeyPath:C:\Users\Christian Encalada\.minikube\machines\minikube\id_rsa Username:docker}
I0804 20:33:34.520200   16908 ssh_runner.go:195] Run: systemctl --version
I0804 20:33:34.539683   16908 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0804 20:33:34.558982   16908 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0804 20:33:34.568208   16908 start.go:410] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0804 20:33:34.582366   16908 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0804 20:33:34.591451   16908 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0804 20:33:34.591451   16908 start.go:466] detecting cgroup driver to use...
I0804 20:33:34.591451   16908 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0804 20:33:34.591451   16908 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0804 20:33:34.619852   16908 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0804 20:33:34.643946   16908 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0804 20:33:34.652937   16908 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0804 20:33:34.668741   16908 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0804 20:33:34.695374   16908 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0804 20:33:34.719912   16908 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0804 20:33:34.743161   16908 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0804 20:33:34.766751   16908 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0804 20:33:34.789676   16908 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0804 20:33:34.814404   16908 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0804 20:33:34.838290   16908 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0804 20:33:34.862304   16908 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0804 20:33:34.979279   16908 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0804 20:33:35.086562   16908 start.go:466] detecting cgroup driver to use...
I0804 20:33:35.086562   16908 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0804 20:33:35.101832   16908 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0804 20:33:35.114520   16908 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0804 20:33:35.130100   16908 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0804 20:33:35.141352   16908 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0804 20:33:35.169833   16908 ssh_runner.go:195] Run: which cri-dockerd
I0804 20:33:35.204149   16908 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0804 20:33:35.212941   16908 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0804 20:33:35.241558   16908 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0804 20:33:35.334510   16908 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0804 20:33:35.448419   16908 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0804 20:33:35.448419   16908 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0804 20:33:35.478982   16908 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0804 20:33:35.579769   16908 ssh_runner.go:195] Run: sudo systemctl restart docker
I0804 20:33:40.331965   16908 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.7521962s)
I0804 20:33:40.347014   16908 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0804 20:33:40.449804   16908 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0804 20:33:40.551250   16908 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0804 20:33:40.670517   16908 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0804 20:33:40.778068   16908 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0804 20:33:40.801971   16908 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0804 20:33:40.918169   16908 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0804 20:33:40.982974   16908 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0804 20:33:40.996903   16908 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0804 20:33:41.000617   16908 start.go:534] Will wait 60s for crictl version
I0804 20:33:41.013787   16908 ssh_runner.go:195] Run: which crictl
I0804 20:33:41.030222   16908 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0804 20:33:41.069628   16908 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0804 20:33:41.078479   16908 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0804 20:33:41.105365   16908 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0804 20:33:41.126013   16908 out.go:204] 🐳  Preparing Kubernetes v1.27.3 on Docker 24.0.4 ...
I0804 20:33:41.135758   16908 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0804 20:33:41.344561   16908 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0804 20:33:41.358233   16908 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0804 20:33:41.362102   16908 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0804 20:33:41.379700   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0804 20:33:41.497789   16908 preload.go:132] Checking if preload exists for k8s version v1.27.3 and runtime docker
I0804 20:33:41.506695   16908 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0804 20:33:41.523106   16908 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0804 20:33:41.523106   16908 docker.go:566] Images already preloaded, skipping extraction
I0804 20:33:41.532058   16908 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0804 20:33:41.547069   16908 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.3
registry.k8s.io/kube-scheduler:v1.27.3
registry.k8s.io/kube-proxy:v1.27.3
registry.k8s.io/kube-controller-manager:v1.27.3
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0804 20:33:41.547069   16908 cache_images.go:84] Images are preloaded, skipping loading
I0804 20:33:41.556016   16908 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0804 20:33:41.603179   16908 cni.go:84] Creating CNI manager for ""
I0804 20:33:41.603179   16908 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0804 20:33:41.603179   16908 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0804 20:33:41.603179   16908 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0804 20:33:41.603724   16908 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0804 20:33:41.603724   16908 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0804 20:33:41.617561   16908 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.3
I0804 20:33:41.627273   16908 binaries.go:44] Found k8s binaries, skipping transfer
I0804 20:33:41.641111   16908 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0804 20:33:41.649215   16908 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0804 20:33:41.662840   16908 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0804 20:33:41.678213   16908 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0804 20:33:41.708574   16908 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0804 20:33:41.712330   16908 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0804 20:33:41.721520   16908 certs.go:56] Setting up C:\Users\Christian Encalada\.minikube\profiles\minikube for IP: 192.168.49.2
I0804 20:33:41.722074   16908 certs.go:190] acquiring lock for shared ca certs: {Name:mkd8547487fd22114e4eda5da9c6dff6efaed57b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0804 20:33:41.722074   16908 certs.go:199] skipping minikubeCA CA generation: C:\Users\Christian Encalada\.minikube\ca.key
I0804 20:33:41.722612   16908 certs.go:199] skipping proxyClientCA CA generation: C:\Users\Christian Encalada\.minikube\proxy-client-ca.key
I0804 20:33:41.723151   16908 certs.go:315] skipping minikube-user signed cert generation: C:\Users\Christian Encalada\.minikube\profiles\minikube\client.key
I0804 20:33:41.723151   16908 certs.go:315] skipping minikube signed cert generation: C:\Users\Christian Encalada\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0804 20:33:41.723151   16908 certs.go:315] skipping aggregator signed cert generation: C:\Users\Christian Encalada\.minikube\profiles\minikube\proxy-client.key
I0804 20:33:41.723692   16908 certs.go:437] found cert: C:\Users\Christian Encalada\.minikube\certs\C:\Users\Christian Encalada\.minikube\certs\ca-key.pem (1675 bytes)
I0804 20:33:41.723692   16908 certs.go:437] found cert: C:\Users\Christian Encalada\.minikube\certs\C:\Users\Christian Encalada\.minikube\certs\ca.pem (1111 bytes)
I0804 20:33:41.724204   16908 certs.go:437] found cert: C:\Users\Christian Encalada\.minikube\certs\C:\Users\Christian Encalada\.minikube\certs\cert.pem (1155 bytes)
I0804 20:33:41.724235   16908 certs.go:437] found cert: C:\Users\Christian Encalada\.minikube\certs\C:\Users\Christian Encalada\.minikube\certs\key.pem (1675 bytes)
I0804 20:33:41.724782   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0804 20:33:41.745186   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0804 20:33:41.765024   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0804 20:33:41.787741   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0804 20:33:41.808008   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0804 20:33:41.827409   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0804 20:33:41.847405   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0804 20:33:41.866851   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0804 20:33:41.889945   16908 ssh_runner.go:362] scp C:\Users\Christian Encalada\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0804 20:33:41.908711   16908 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0804 20:33:41.936755   16908 ssh_runner.go:195] Run: openssl version
I0804 20:33:41.954898   16908 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0804 20:33:41.975786   16908 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0804 20:33:41.979656   16908 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Aug  5 00:40 /usr/share/ca-certificates/minikubeCA.pem
I0804 20:33:41.993530   16908 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0804 20:33:42.014272   16908 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0804 20:33:42.035328   16908 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0804 20:33:42.052043   16908 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0804 20:33:42.071108   16908 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0804 20:33:42.089584   16908 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0804 20:33:42.109627   16908 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0804 20:33:42.129415   16908 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0804 20:33:42.148149   16908 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0804 20:33:42.153867   16908 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Christian Encalada:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0804 20:33:42.162293   16908 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0804 20:33:42.191861   16908 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0804 20:33:42.199807   16908 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0804 20:33:42.199807   16908 kubeadm.go:636] restartCluster start
I0804 20:33:42.213501   16908 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0804 20:33:42.220984   16908 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0804 20:33:42.230099   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0804 20:33:42.347232   16908 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in C:\Users\Christian Encalada\.kube\config
I0804 20:33:42.347756   16908 kubeconfig.go:146] "minikube" context is missing from C:\Users\Christian Encalada\.kube\config - will repair!
I0804 20:33:42.347756   16908 lock.go:35] WriteFile acquiring C:\Users\Christian Encalada\.kube\config: {Name:mk283298194cf46b21677043ed106948eb8a9a54 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0804 20:33:42.370260   16908 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0804 20:33:42.377977   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:42.391064   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:42.398990   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:42.910982   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:42.924023   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:42.932500   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:43.410120   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:43.426014   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:43.435582   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:43.900473   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:43.914107   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:43.923296   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:44.403960   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:44.418611   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:44.427640   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:44.908474   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:44.921790   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:44.930490   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:45.409300   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:45.423261   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:45.432337   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:45.911717   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:45.925033   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:45.934485   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:46.410626   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:46.424189   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:46.433342   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:46.911047   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:46.924673   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:46.933227   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:47.409079   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:47.422091   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:47.430545   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:47.909959   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:47.924307   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:47.932803   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:48.410041   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:48.424834   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:48.434000   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:48.913084   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:48.926126   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:48.934996   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:49.400223   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:49.416418   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:49.427452   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:49.903578   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:49.917208   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:49.926278   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:50.408681   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:50.422982   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:50.433212   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:50.899629   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:50.912729   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:50.921660   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:51.415019   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:51.429835   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:51.439384   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:51.913733   16908 api_server.go:166] Checking apiserver status ...
I0804 20:33:51.926902   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0804 20:33:51.935952   16908 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0804 20:33:52.382778   16908 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0804 20:33:52.382778   16908 kubeadm.go:1128] stopping kube-system containers ...
I0804 20:33:52.392560   16908 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0804 20:33:52.413316   16908 docker.go:462] Stopping containers: [5cf61b91cde7 73acc1c5ea1e e20452fd2cf2 826d65d4617c 1dbdd5195d32 0efbbdcbceb2 d4169f52777e e809798c4c82 ab1ca40f9f3e 2e4eac942a3a 8657c4cac4e0 6c1876f82b8b 3f8524d032bb d620a3090125 48a6b14efc27 5172fb51adf6 a1a2be8bcd4f 68bf784fed46 723d093ff3f2 85a86915bcab 27aa9cdefe6a 246c4f89b14f b91bd539386c 279956d92d75 ef36512385f5 02d1bce57d6f 84d2ac2c74b5]
I0804 20:33:52.421824   16908 ssh_runner.go:195] Run: docker stop 5cf61b91cde7 73acc1c5ea1e e20452fd2cf2 826d65d4617c 1dbdd5195d32 0efbbdcbceb2 d4169f52777e e809798c4c82 ab1ca40f9f3e 2e4eac942a3a 8657c4cac4e0 6c1876f82b8b 3f8524d032bb d620a3090125 48a6b14efc27 5172fb51adf6 a1a2be8bcd4f 68bf784fed46 723d093ff3f2 85a86915bcab 27aa9cdefe6a 246c4f89b14f b91bd539386c 279956d92d75 ef36512385f5 02d1bce57d6f 84d2ac2c74b5
I0804 20:33:52.452905   16908 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0804 20:33:52.476758   16908 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0804 20:33:52.485310   16908 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Aug  5 01:09 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Aug  5 01:09 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 Aug  5 01:09 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Aug  5 01:09 /etc/kubernetes/scheduler.conf

I0804 20:33:52.499836   16908 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0804 20:33:52.520722   16908 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0804 20:33:52.546147   16908 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0804 20:33:52.554907   16908 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0804 20:33:52.569870   16908 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0804 20:33:52.591659   16908 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0804 20:33:52.598698   16908 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0804 20:33:52.611699   16908 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0804 20:33:52.633699   16908 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0804 20:33:52.640700   16908 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0804 20:33:52.640700   16908 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0804 20:33:52.682106   16908 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0804 20:33:53.514614   16908 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0804 20:33:53.650696   16908 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0804 20:33:53.697694   16908 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0804 20:33:53.786782   16908 api_server.go:52] waiting for apiserver process to appear ...
I0804 20:33:53.805993   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0804 20:33:54.337480   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0804 20:33:54.837489   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0804 20:33:55.335551   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0804 20:33:55.836545   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0804 20:33:55.895108   16908 api_server.go:72] duration metric: took 2.1083256s to wait for apiserver process to appear ...
I0804 20:33:55.895108   16908 api_server.go:88] waiting for apiserver healthz status ...
I0804 20:33:55.895108   16908 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53725/healthz ...
I0804 20:33:58.338056   16908 api_server.go:279] https://127.0.0.1:53725/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0804 20:33:58.338056   16908 api_server.go:103] status: https://127.0.0.1:53725/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0804 20:33:58.845565   16908 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53725/healthz ...
I0804 20:33:58.852708   16908 api_server.go:279] https://127.0.0.1:53725/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0804 20:33:58.852708   16908 api_server.go:103] status: https://127.0.0.1:53725/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0804 20:33:59.345168   16908 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53725/healthz ...
I0804 20:33:59.352283   16908 api_server.go:279] https://127.0.0.1:53725/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0804 20:33:59.352283   16908 api_server.go:103] status: https://127.0.0.1:53725/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0804 20:33:59.844474   16908 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53725/healthz ...
I0804 20:33:59.894196   16908 api_server.go:279] https://127.0.0.1:53725/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0804 20:33:59.894196   16908 api_server.go:103] status: https://127.0.0.1:53725/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0804 20:34:00.346095   16908 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53725/healthz ...
I0804 20:34:00.352924   16908 api_server.go:279] https://127.0.0.1:53725/healthz returned 200:
ok
I0804 20:34:00.363834   16908 api_server.go:141] control plane version: v1.27.3
I0804 20:34:00.363834   16908 api_server.go:131] duration metric: took 4.4687259s to wait for apiserver health ...
I0804 20:34:00.363834   16908 cni.go:84] Creating CNI manager for ""
I0804 20:34:00.363834   16908 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0804 20:34:00.365941   16908 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0804 20:34:00.381475   16908 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0804 20:34:00.391121   16908 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0804 20:34:00.405932   16908 system_pods.go:43] waiting for kube-system pods to appear ...
I0804 20:34:00.415860   16908 system_pods.go:59] 7 kube-system pods found
I0804 20:34:00.415860   16908 system_pods.go:61] "coredns-5d78c9869d-pjtwl" [3ae42aea-208a-47dc-b365-171b2d6a894d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0804 20:34:00.415860   16908 system_pods.go:61] "etcd-minikube" [dbd30dfd-c371-45ea-87c9-767ff3571a00] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0804 20:34:00.415860   16908 system_pods.go:61] "kube-apiserver-minikube" [03499214-9044-4637-b2a9-b4ac0e5204df] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0804 20:34:00.415860   16908 system_pods.go:61] "kube-controller-manager-minikube" [48f10a37-60d9-4351-863f-0fc852438df7] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0804 20:34:00.415860   16908 system_pods.go:61] "kube-proxy-flwvv" [a415efe5-97b4-4e39-b34b-ada61d938539] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0804 20:34:00.415860   16908 system_pods.go:61] "kube-scheduler-minikube" [be11be87-ab13-4b65-b2b3-b0f5a4b5c2f1] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0804 20:34:00.415860   16908 system_pods.go:61] "storage-provisioner" [53cdd0ad-d4e8-433f-92f1-618e7ce9b9e0] Running
I0804 20:34:00.415860   16908 system_pods.go:74] duration metric: took 9.9286ms to wait for pod list to return data ...
I0804 20:34:00.415860   16908 node_conditions.go:102] verifying NodePressure condition ...
I0804 20:34:00.420161   16908 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0804 20:34:00.420161   16908 node_conditions.go:123] node cpu capacity is 12
I0804 20:34:00.420161   16908 node_conditions.go:105] duration metric: took 4.3003ms to run NodePressure ...
I0804 20:34:00.420161   16908 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0804 20:34:00.636925   16908 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0804 20:34:00.642909   16908 ops.go:34] apiserver oom_adj: -16
I0804 20:34:00.642909   16908 kubeadm.go:640] restartCluster took 18.4431013s
I0804 20:34:00.642909   16908 kubeadm.go:406] StartCluster complete in 18.4890414s
I0804 20:34:00.642909   16908 settings.go:142] acquiring lock: {Name:mk12825fef316ec9772439b1273acb645d506381 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0804 20:34:00.642909   16908 settings.go:150] Updating kubeconfig:  C:\Users\Christian Encalada\.kube\config
I0804 20:34:00.643445   16908 lock.go:35] WriteFile acquiring C:\Users\Christian Encalada\.kube\config: {Name:mk283298194cf46b21677043ed106948eb8a9a54 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0804 20:34:00.644472   16908 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0804 20:34:00.644518   16908 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0804 20:34:00.644518   16908 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0804 20:34:00.644518   16908 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0804 20:34:00.644518   16908 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0804 20:34:00.644518   16908 addons.go:240] addon storage-provisioner should already be in state true
I0804 20:34:00.644518   16908 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0804 20:34:00.644518   16908 host.go:66] Checking if "minikube" exists ...
I0804 20:34:00.644518   16908 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.3
I0804 20:34:00.667789   16908 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0804 20:34:00.667789   16908 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0804 20:34:00.669508   16908 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0804 20:34:00.669508   16908 out.go:177] 🔎  Verifying Kubernetes components...
I0804 20:34:00.670043   16908 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0804 20:34:00.689475   16908 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0804 20:34:00.721283   16908 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0804 20:34:00.731542   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0804 20:34:00.802585   16908 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0804 20:34:00.802585   16908 addons.go:240] addon default-storageclass should already be in state true
I0804 20:34:00.802585   16908 host.go:66] Checking if "minikube" exists ...
I0804 20:34:00.811584   16908 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0804 20:34:00.812582   16908 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0804 20:34:00.812582   16908 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0804 20:34:00.822584   16908 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0804 20:34:00.823584   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:34:00.857582   16908 api_server.go:52] waiting for apiserver process to appear ...
I0804 20:34:00.871583   16908 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0804 20:34:00.881586   16908 api_server.go:72] duration metric: took 213.7973ms to wait for apiserver process to appear ...
I0804 20:34:00.881586   16908 api_server.go:88] waiting for apiserver healthz status ...
I0804 20:34:00.881586   16908 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53725/healthz ...
I0804 20:34:00.889584   16908 api_server.go:279] https://127.0.0.1:53725/healthz returned 200:
ok
I0804 20:34:00.891585   16908 api_server.go:141] control plane version: v1.27.3
I0804 20:34:00.891585   16908 api_server.go:131] duration metric: took 9.9992ms to wait for apiserver health ...
I0804 20:34:00.891585   16908 system_pods.go:43] waiting for kube-system pods to appear ...
I0804 20:34:00.898583   16908 system_pods.go:59] 7 kube-system pods found
I0804 20:34:00.898583   16908 system_pods.go:61] "coredns-5d78c9869d-pjtwl" [3ae42aea-208a-47dc-b365-171b2d6a894d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0804 20:34:00.898583   16908 system_pods.go:61] "etcd-minikube" [dbd30dfd-c371-45ea-87c9-767ff3571a00] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0804 20:34:00.898583   16908 system_pods.go:61] "kube-apiserver-minikube" [03499214-9044-4637-b2a9-b4ac0e5204df] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0804 20:34:00.898583   16908 system_pods.go:61] "kube-controller-manager-minikube" [48f10a37-60d9-4351-863f-0fc852438df7] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0804 20:34:00.898583   16908 system_pods.go:61] "kube-proxy-flwvv" [a415efe5-97b4-4e39-b34b-ada61d938539] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0804 20:34:00.898583   16908 system_pods.go:61] "kube-scheduler-minikube" [be11be87-ab13-4b65-b2b3-b0f5a4b5c2f1] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0804 20:34:00.898583   16908 system_pods.go:61] "storage-provisioner" [53cdd0ad-d4e8-433f-92f1-618e7ce9b9e0] Running
I0804 20:34:00.898583   16908 system_pods.go:74] duration metric: took 6.9981ms to wait for pod list to return data ...
I0804 20:34:00.898583   16908 kubeadm.go:581] duration metric: took 230.7946ms to wait for : map[apiserver:true system_pods:true] ...
I0804 20:34:00.898583   16908 node_conditions.go:102] verifying NodePressure condition ...
I0804 20:34:00.902585   16908 node_conditions.go:122] node storage ephemeral capacity is 263174212Ki
I0804 20:34:00.902585   16908 node_conditions.go:123] node cpu capacity is 12
I0804 20:34:00.902585   16908 node_conditions.go:105] duration metric: took 4.0022ms to run NodePressure ...
I0804 20:34:00.902585   16908 start.go:228] waiting for startup goroutines ...
I0804 20:34:00.952681   16908 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53726 SSHKeyPath:C:\Users\Christian Encalada\.minikube\machines\minikube\id_rsa Username:docker}
I0804 20:34:00.962963   16908 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0804 20:34:00.962963   16908 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0804 20:34:00.971546   16908 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0804 20:34:01.024007   16908 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0804 20:34:01.104960   16908 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53726 SSHKeyPath:C:\Users\Christian Encalada\.minikube\machines\minikube\id_rsa Username:docker}
I0804 20:34:01.224741   16908 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0804 20:34:01.900486   16908 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0804 20:34:01.904213   16908 addons.go:502] enable addons completed in 1.259695s: enabled=[storage-provisioner default-storageclass]
I0804 20:34:01.904213   16908 start.go:233] waiting for cluster config update ...
I0804 20:34:01.904213   16908 start.go:242] writing updated cluster config ...
I0804 20:34:01.918917   16908 ssh_runner.go:195] Run: rm -f paused
I0804 20:34:02.004553   16908 start.go:596] kubectl: 1.27.2, cluster: 1.27.3 (minor skew: 0)
I0804 20:34:02.009707   16908 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Aug 05 01:33:37 minikube systemd[1]: Stopped Docker Application Container Engine.
Aug 05 01:33:37 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 05 01:33:37 minikube dockerd[962]: time="2023-08-05T01:33:37.684915466Z" level=info msg="Starting up"
Aug 05 01:33:38 minikube dockerd[962]: time="2023-08-05T01:33:38.019666495Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Aug 05 01:33:38 minikube dockerd[962]: time="2023-08-05T01:33:38.035437852Z" level=info msg="Loading containers: start."
Aug 05 01:33:39 minikube dockerd[962]: time="2023-08-05T01:33:39.590227210Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Aug 05 01:33:40 minikube dockerd[962]: time="2023-08-05T01:33:40.272605883Z" level=info msg="Loading containers: done."
Aug 05 01:33:40 minikube dockerd[962]: time="2023-08-05T01:33:40.301503253Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Aug 05 01:33:40 minikube dockerd[962]: time="2023-08-05T01:33:40.301584735Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Aug 05 01:33:40 minikube dockerd[962]: time="2023-08-05T01:33:40.301593812Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Aug 05 01:33:40 minikube dockerd[962]: time="2023-08-05T01:33:40.301597790Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Aug 05 01:33:40 minikube dockerd[962]: time="2023-08-05T01:33:40.301614491Z" level=info msg="Docker daemon" commit=4ffc614 graphdriver=overlay2 version=24.0.4
Aug 05 01:33:40 minikube dockerd[962]: time="2023-08-05T01:33:40.301659606Z" level=info msg="Daemon has completed initialization"
Aug 05 01:33:40 minikube dockerd[962]: time="2023-08-05T01:33:40.329526334Z" level=info msg="API listen on /var/run/docker.sock"
Aug 05 01:33:40 minikube dockerd[962]: time="2023-08-05T01:33:40.329587178Z" level=info msg="API listen on [::]:2376"
Aug 05 01:33:40 minikube systemd[1]: Started Docker Application Container Engine.
Aug 05 01:33:40 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Start docker client with request timeout 0s"
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Hairpin mode is set to hairpin-veth"
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Loaded network plugin cni"
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Docker cri networking managed by network plugin cni"
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Docker Info: &{ID:d60ba691-d309-4c0c-b2e9-363c8c49f443 Containers:29 ContainersRunning:0 ContainersPaused:0 ContainersStopped:29 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:35 SystemTime:2023-08-05T01:33:40.972803589Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.10.16.3-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.2 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc00065e0e0 NCPU:12 MemTotal:13343838208 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Setting cgroupDriver cgroupfs"
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Aug 05 01:33:40 minikube cri-dockerd[1207]: time="2023-08-05T01:33:40Z" level=info msg="Start cri-dockerd grpc backend"
Aug 05 01:33:40 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Aug 05 01:33:54 minikube cri-dockerd[1207]: time="2023-08-05T01:33:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hola-mundo-deployment-78475ff4cb-sh5cc_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"15d5cceac07c698de5cb5a8870911e879d215db7f6f7408ed944fafc7c8e454d\""
Aug 05 01:33:54 minikube cri-dockerd[1207]: time="2023-08-05T01:33:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-pjtwl_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ab1ca40f9f3ea20434ed7216233e99e585d3c75308d636bd32e044b32993f368\""
Aug 05 01:33:54 minikube cri-dockerd[1207]: time="2023-08-05T01:33:54Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hola-mundo-deployment-78475ff4cb-dfswz_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a041af3e0cfad83ccf65d95ba8ac8b289ef038ab5e9615278c5feb64bdff561c\""
Aug 05 01:33:55 minikube cri-dockerd[1207]: time="2023-08-05T01:33:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a2425cec88efbf4f89dd587e8abad12edc5d8ad638af93dc3f4f2da68056c778/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 01:33:55 minikube cri-dockerd[1207]: time="2023-08-05T01:33:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cdd3bdfc8777155b9417e30b80688c9c874c60197c0f83e660124efab912179a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 01:33:55 minikube cri-dockerd[1207]: time="2023-08-05T01:33:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5baf9baf0441946d29f3f955e0a9564696f7db8e279241750b310d5f00337aa9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 01:33:55 minikube cri-dockerd[1207]: time="2023-08-05T01:33:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1dc96c81ff78581e0c0ea02ae88d832093cb5c9a48c159f4ac27b55a02527ffc/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 01:33:58 minikube cri-dockerd[1207]: time="2023-08-05T01:33:58Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 05 01:33:59 minikube cri-dockerd[1207]: time="2023-08-05T01:33:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6fb0a50813e1fc974806345ce9f6e8d5514a3933252913531134447def83c76e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 01:33:59 minikube cri-dockerd[1207]: time="2023-08-05T01:33:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/546671b6f0782ea0e8b4d05390082a4687d39f9341c9b2c18ef756f8fd66691d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 01:33:59 minikube cri-dockerd[1207]: time="2023-08-05T01:33:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8e026db0cc0d460292807df87f9edd4a6a32d136ec0c9fe8355473b054e9e9c4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 05 01:33:59 minikube cri-dockerd[1207]: time="2023-08-05T01:33:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/15889ac46388e805aed57b2fe1728c14213d3dc58ae03668405d04b1aabcc68f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 05 01:34:00 minikube cri-dockerd[1207]: time="2023-08-05T01:34:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5ea6a57e1db2ba4d44a2e599140325ec9e38a327ad8b60940c40082e1459dc76/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 05 01:34:02 minikube dockerd[962]: time="2023-08-05T01:34:02.970192848Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 05 01:34:02 minikube dockerd[962]: time="2023-08-05T01:34:02.970250265Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 05 01:34:04 minikube dockerd[962]: time="2023-08-05T01:34:04.621416137Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 05 01:34:04 minikube dockerd[962]: time="2023-08-05T01:34:04.621486979Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 05 01:34:10 minikube dockerd[962]: time="2023-08-05T01:34:10.218871489Z" level=info msg="ignoring event" container=9a6f534d8270cb245cba4a395dc60e98aa3b8d93a60a647abd0f65b6f45587e6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 05 01:34:16 minikube dockerd[962]: time="2023-08-05T01:34:16.308343808Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 05 01:34:16 minikube dockerd[962]: time="2023-08-05T01:34:16.308394804Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 05 01:34:18 minikube dockerd[962]: time="2023-08-05T01:34:18.060016457Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 05 01:34:18 minikube dockerd[962]: time="2023-08-05T01:34:18.060068244Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 05 01:34:38 minikube dockerd[962]: time="2023-08-05T01:34:38.618819635Z" level=info msg="ignoring event" container=8e026db0cc0d460292807df87f9edd4a6a32d136ec0c9fe8355473b054e9e9c4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 05 01:34:38 minikube cri-dockerd[1207]: time="2023-08-05T01:34:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/29f25b1bd5f83a78208126992325cca3232e4c65fc208215f50f171227210e4b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 05 01:34:41 minikube dockerd[962]: time="2023-08-05T01:34:41.058695717Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 05 01:34:41 minikube dockerd[962]: time="2023-08-05T01:34:41.058766009Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 05 01:34:44 minikube dockerd[962]: time="2023-08-05T01:34:44.913711983Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 05 01:34:44 minikube dockerd[962]: time="2023-08-05T01:34:44.913770453Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Aug 05 01:34:44 minikube cri-dockerd[1207]: time="2023-08-05T01:34:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4bd7e1b212191e6680410026c2532e031aa731f8f6fa4431d3e287225b294c07/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 05 01:34:46 minikube dockerd[962]: time="2023-08-05T01:34:46.276957399Z" level=info msg="ignoring event" container=5ea6a57e1db2ba4d44a2e599140325ec9e38a327ad8b60940c40082e1459dc76 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 05 01:34:46 minikube dockerd[962]: time="2023-08-05T01:34:46.761674929Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Aug 05 01:34:46 minikube dockerd[962]: time="2023-08-05T01:34:46.761751783Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
1f54660d924af       6e38f40d628db       27 seconds ago      Running             storage-provisioner       9                   546671b6f0782       storage-provisioner
58d037e0d3be1       ead0a4a53df89       51 seconds ago      Running             coredns                   4                   15889ac46388e       coredns-5d78c9869d-pjtwl
9a6f534d8270c       6e38f40d628db       52 seconds ago      Exited              storage-provisioner       8                   546671b6f0782       storage-provisioner
5c22cca55d957       5780543258cf0       52 seconds ago      Running             kube-proxy                4                   6fb0a50813e1f       kube-proxy-flwvv
418cab48a7f55       41697ceeb70b3       56 seconds ago      Running             kube-scheduler            4                   cdd3bdfc87771       kube-scheduler-minikube
74e16f5d708c6       08a0c939e61b7       56 seconds ago      Running             kube-apiserver            4                   5baf9baf04419       kube-apiserver-minikube
84ef6d1381e19       7cffc01dba0e1       56 seconds ago      Running             kube-controller-manager   4                   1dc96c81ff785       kube-controller-manager-minikube
36da9f099a658       86b6af7dd652c       56 seconds ago      Running             etcd                      4                   a2425cec88efb       etcd-minikube
73acc1c5ea1eb       ead0a4a53df89       5 minutes ago       Exited              coredns                   3                   ab1ca40f9f3ea       coredns-5d78c9869d-pjtwl
e20452fd2cf28       86b6af7dd652c       5 minutes ago       Exited              etcd                      3                   6c1876f82b8b6       etcd-minikube
826d65d4617c7       5780543258cf0       5 minutes ago       Exited              kube-proxy                3                   48a6b14efc273       kube-proxy-flwvv
0efbbdcbceb29       41697ceeb70b3       5 minutes ago       Exited              kube-scheduler            3                   8657c4cac4e05       kube-scheduler-minikube
d4169f52777e9       08a0c939e61b7       5 minutes ago       Exited              kube-apiserver            3                   3f8524d032bb1       kube-apiserver-minikube
e809798c4c822       7cffc01dba0e1       5 minutes ago       Exited              kube-controller-manager   3                   d620a30901250       kube-controller-manager-minikube

* 
* ==> coredns [58d037e0d3be] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:46235 - 6638 "HINFO IN 7133724529097225799.2222111926643261207. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.223887474s
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [73acc1c5ea1e] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:54121 - 63129 "HINFO IN 4968189677634559148.2838715069597965074. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.292658388s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd3f3801765d093a485d255043149f92ec0a695f
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_08_04T19_40_49_0700
                    minikube.k8s.io/version=v1.31.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 05 Aug 2023 00:40:43 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 05 Aug 2023 01:34:49 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 05 Aug 2023 01:33:58 +0000   Sat, 05 Aug 2023 00:40:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 05 Aug 2023 01:33:58 +0000   Sat, 05 Aug 2023 00:40:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 05 Aug 2023 01:33:58 +0000   Sat, 05 Aug 2023 00:40:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 05 Aug 2023 01:33:58 +0000   Sat, 05 Aug 2023 00:40:44 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             13031092Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  263174212Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             13031092Ki
  pods:               110
System Info:
  Machine ID:                 ee685559c412481ca9ab025d6f58d546
  System UUID:                ee685559c412481ca9ab025d6f58d546
  Boot ID:                    4186f160-a24d-4dac-b06b-bff102ff79d1
  Kernel Version:             5.10.16.3-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.3
  Kube-Proxy Version:         v1.27.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                      CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                      ------------  ----------  ---------------  -------------  ---
  default                     hola-mundo-deployment-78475ff4cb-dlxxd    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13s
  default                     hola-mundo-deployment-78475ff4cb-nwnkn    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7s
  kube-system                 coredns-5d78c9869d-pjtwl                  100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     53m
  kube-system                 etcd-minikube                             100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         54m
  kube-system                 kube-apiserver-minikube                   250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         54m
  kube-system                 kube-controller-manager-minikube          200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         54m
  kube-system                 kube-proxy-flwvv                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         53m
  kube-system                 kube-scheduler-minikube                   100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         54m
  kube-system                 storage-provisioner                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         54m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (1%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 53m                kube-proxy       
  Normal   Starting                 50s                kube-proxy       
  Normal   Starting                 5m1s               kube-proxy       
  Normal   Starting                 6m11s              kube-proxy       
  Normal   Starting                 25m                kube-proxy       
  Normal   Starting                 54m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientPID     54m (x7 over 54m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure    54m (x8 over 54m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeAllocatableEnforced  54m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  54m (x8 over 54m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                 54m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  54m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    54m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     54m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  54m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           53m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 25m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  25m (x8 over 25m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    25m (x8 over 25m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     25m (x7 over 25m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  25m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           25m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode           5m59s              node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeNotReady             5m20s              kubelet          Node minikube status is now: NodeNotReady
  Warning  ContainerGCFailed        5m15s              kubelet          [rpc error: code = Unknown desc = error during connect: Get "http://%!!(MISSING)F(MISSING)var%!!(MISSING)F(MISSING)run%!!(MISSING)F(MISSING)docker.sock/v1.42/containers/json?all=1&filters=%!!(MISSING)B(MISSING)%!!(MISSING)l(MISSING)abel%!A(MISSING)%!!(MISSING)B(MISSING)%!!(MISSING)i(MISSING)o.kubernetes.docker.type%!!(MISSING)D(MISSING)container%!A(MISSING)true%!!(MISSING)D(MISSING)%!!(MISSING)D(MISSING)": read unix @->/var/run/docker.sock: read: connection reset by peer, rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?]
  Normal   RegisteredNode           4m49s              node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 58s                kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  58s                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  57s (x8 over 58s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    57s (x8 over 58s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     57s (x7 over 58s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode           41s                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.000055] init: (1) ERROR: UtilCreateProcessAndWait:501: /bin/mount failed with status 0x
[  +0.000001] ff00
[  +0.000004] init: (1) ERROR: MountPlan9:493: mount cache=mmap,noatime,trans=fd,rfdno=8,wfdno=8,msize=65536,aname=drvfs;path=C:\;uid=0;gid=0;symlinkroot=/mnt/
[Aug 5 00:36] WSL2: Performing memory compaction.
[Aug 5 00:37] WSL2: Performing memory compaction.
[Aug 5 00:38] WSL2: Performing memory compaction.
[Aug 5 00:39] WSL2: Performing memory compaction.
[Aug 5 00:40] WSL2: Performing memory compaction.
[Aug 5 00:41] WSL2: Performing memory compaction.
[Aug 5 00:42] WSL2: Performing memory compaction.
[Aug 5 00:43] WSL2: Performing memory compaction.
[Aug 5 00:44] WSL2: Performing memory compaction.
[Aug 5 00:45] WSL2: Performing memory compaction.
[Aug 5 00:46] WSL2: Performing memory compaction.
[Aug 5 00:47] WSL2: Performing memory compaction.
[Aug 5 00:48] WSL2: Performing memory compaction.
[Aug 5 00:49] WSL2: Performing memory compaction.
[Aug 5 00:50] WSL2: Performing memory compaction.
[Aug 5 00:51] WSL2: Performing memory compaction.
[Aug 5 00:52] WSL2: Performing memory compaction.
[Aug 5 00:53] WSL2: Performing memory compaction.
[Aug 5 00:54] WSL2: Performing memory compaction.
[Aug 5 00:55] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[ +28.325996] WSL2: Performing memory compaction.
[Aug 5 00:56] WSL2: Performing memory compaction.
[Aug 5 00:59] WSL2: Performing memory compaction.
[Aug 5 01:01] WSL2: Performing memory compaction.
[Aug 5 01:02] WSL2: Performing memory compaction.
[Aug 5 01:03] WSL2: Performing memory compaction.
[Aug 5 01:04] WSL2: Performing memory compaction.
[Aug 5 01:05] WSL2: Performing memory compaction.
[Aug 5 01:06] WSL2: Performing memory compaction.
[Aug 5 01:07] WSL2: Performing memory compaction.
[Aug 5 01:08] WSL2: Performing memory compaction.
[Aug 5 01:09] WSL2: Performing memory compaction.
[Aug 5 01:10] WSL2: Performing memory compaction.
[Aug 5 01:11] WSL2: Performing memory compaction.
[Aug 5 01:12] WSL2: Performing memory compaction.
[Aug 5 01:13] WSL2: Performing memory compaction.
[Aug 5 01:14] WSL2: Performing memory compaction.
[Aug 5 01:15] WSL2: Performing memory compaction.
[Aug 5 01:16] WSL2: Performing memory compaction.
[Aug 5 01:17] WSL2: Performing memory compaction.
[Aug 5 01:18] WSL2: Performing memory compaction.
[Aug 5 01:19] WSL2: Performing memory compaction.
[Aug 5 01:20] WSL2: Performing memory compaction.
[Aug 5 01:21] WSL2: Performing memory compaction.
[Aug 5 01:22] WSL2: Performing memory compaction.
[Aug 5 01:23] WSL2: Performing memory compaction.
[Aug 5 01:24] WSL2: Performing memory compaction.
[Aug 5 01:25] WSL2: Performing memory compaction.
[Aug 5 01:26] WSL2: Performing memory compaction.
[Aug 5 01:27] WSL2: Performing memory compaction.
[Aug 5 01:28] WSL2: Performing memory compaction.
[Aug 5 01:29] WSL2: Performing memory compaction.
[Aug 5 01:30] WSL2: Performing memory compaction.
[Aug 5 01:31] WSL2: Performing memory compaction.
[Aug 5 01:32] WSL2: Performing memory compaction.
[Aug 5 01:33] WSL2: Performing memory compaction.
[Aug 5 01:34] WSL2: Performing memory compaction.

* 
* ==> etcd [36da9f099a65] <==
* {"level":"info","ts":"2023-08-05T01:33:55.600Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-08-05T01:33:55.601Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-08-05T01:33:55.601Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-05T01:33:55.601Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-05T01:33:55.601Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-08-05T01:33:55.601Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-08-05T01:33:55.603Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.825589ms"}
{"level":"info","ts":"2023-08-05T01:33:55.611Z","caller":"etcdserver/server.go:530","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2023-08-05T01:33:55.694Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":2959}
{"level":"info","ts":"2023-08-05T01:33:55.695Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2023-08-05T01:33:55.695Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 5"}
{"level":"info","ts":"2023-08-05T01:33:55.695Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 5, commit: 2959, applied: 0, lastindex: 2959, lastterm: 5]"}
{"level":"warn","ts":"2023-08-05T01:33:55.699Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-08-05T01:33:55.700Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1715}
{"level":"info","ts":"2023-08-05T01:33:55.706Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":2468}
{"level":"info","ts":"2023-08-05T01:33:55.784Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-08-05T01:33:55.788Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-05T01:33:55.789Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-05T01:33:55.791Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-05T01:33:55.791Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-05T01:33:55.791Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-08-05T01:33:55.791Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-05T01:33:55.791Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-08-05T01:33:57.295Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2023-08-05T01:33:57.296Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2023-08-05T01:33:57.296Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2023-08-05T01:33:57.296Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2023-08-05T01:33:57.296Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2023-08-05T01:33:57.296Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2023-08-05T01:33:57.296Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2023-08-05T01:33:57.301Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-08-05T01:33:57.301Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-05T01:33:57.301Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-05T01:33:57.302Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-08-05T01:33:57.302Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-08-05T01:33:57.302Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-08-05T01:33:57.302Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-08-05T01:34:04.945Z","caller":"traceutil/trace.go:171","msg":"trace[1927347990] transaction","detail":"{read_only:false; response_revision:2552; number_of_response:1; }","duration":"155.73594ms","start":"2023-08-05T01:34:04.789Z","end":"2023-08-05T01:34:04.945Z","steps":["trace[1927347990] 'process raft request'  (duration: 155.629611ms)"],"step_count":1}
{"level":"info","ts":"2023-08-05T01:34:05.101Z","caller":"traceutil/trace.go:171","msg":"trace[168768345] transaction","detail":"{read_only:false; response_revision:2553; number_of_response:1; }","duration":"153.606863ms","start":"2023-08-05T01:34:04.948Z","end":"2023-08-05T01:34:05.101Z","steps":["trace[168768345] 'process raft request'  (duration: 149.104361ms)"],"step_count":1}
{"level":"info","ts":"2023-08-05T01:34:05.579Z","caller":"traceutil/trace.go:171","msg":"trace[1651668609] transaction","detail":"{read_only:false; response_revision:2555; number_of_response:1; }","duration":"142.116525ms","start":"2023-08-05T01:34:05.437Z","end":"2023-08-05T01:34:05.579Z","steps":["trace[1651668609] 'process raft request'  (duration: 141.223532ms)"],"step_count":1}
{"level":"info","ts":"2023-08-05T01:34:42.132Z","caller":"traceutil/trace.go:171","msg":"trace[1968877958] transaction","detail":"{read_only:false; response_revision:2634; number_of_response:1; }","duration":"304.548546ms","start":"2023-08-05T01:34:41.827Z","end":"2023-08-05T01:34:42.132Z","steps":["trace[1968877958] 'process raft request'  (duration: 304.394889ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-05T01:34:42.132Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-05T01:34:41.827Z","time spent":"304.692235ms","remote":"127.0.0.1:39990","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":705,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/default/hola-mundo-deployment-78475ff4cb-dlxxd.177858c955a35636\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/hola-mundo-deployment-78475ff4cb-dlxxd.177858c955a35636\" value_size:607 lease:8128022899524530294 >> failure:<>"}
{"level":"info","ts":"2023-08-05T01:34:42.136Z","caller":"traceutil/trace.go:171","msg":"trace[259991005] transaction","detail":"{read_only:false; response_revision:2635; number_of_response:1; }","duration":"306.374646ms","start":"2023-08-05T01:34:41.830Z","end":"2023-08-05T01:34:42.136Z","steps":["trace[259991005] 'process raft request'  (duration: 305.994875ms)"],"step_count":1}
{"level":"warn","ts":"2023-08-05T01:34:42.136Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"250.422089ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2023-08-05T01:34:42.136Z","caller":"traceutil/trace.go:171","msg":"trace[419037178] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:2635; }","duration":"250.464739ms","start":"2023-08-05T01:34:41.886Z","end":"2023-08-05T01:34:42.136Z","steps":["trace[419037178] 'agreement among raft nodes before linearized reading'  (duration: 250.391041ms)"],"step_count":1}
{"level":"info","ts":"2023-08-05T01:34:42.136Z","caller":"traceutil/trace.go:171","msg":"trace[1470728355] linearizableReadLoop","detail":"{readStateIndex:3151; appliedIndex:3149; }","duration":"250.189073ms","start":"2023-08-05T01:34:41.886Z","end":"2023-08-05T01:34:42.136Z","steps":["trace[1470728355] 'read index received'  (duration: 245.969785ms)","trace[1470728355] 'applied index is now lower than readState.Index'  (duration: 4.218607ms)"],"step_count":2}
{"level":"warn","ts":"2023-08-05T01:34:42.136Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-08-05T01:34:41.830Z","time spent":"306.490693ms","remote":"127.0.0.1:40010","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":2884,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/default/hola-mundo-deployment-78475ff4cb-dlxxd\" mod_revision:2618 > success:<request_put:<key:\"/registry/pods/default/hola-mundo-deployment-78475ff4cb-dlxxd\" value_size:2815 >> failure:<request_range:<key:\"/registry/pods/default/hola-mundo-deployment-78475ff4cb-dlxxd\" > >"}

* 
* ==> etcd [e20452fd2cf2] <==
* {"level":"info","ts":"2023-08-05T01:29:47.804Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-08-05T01:29:47.804Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-08-05T01:29:47.804Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-05T01:29:47.804Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-05T01:29:47.805Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-08-05T01:29:47.805Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-08-05T01:29:47.885Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"79.531315ms"}
{"level":"info","ts":"2023-08-05T01:29:47.903Z","caller":"etcdserver/server.go:530","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2023-08-05T01:29:47.988Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":2721}
{"level":"info","ts":"2023-08-05T01:29:47.988Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2023-08-05T01:29:47.989Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2023-08-05T01:29:47.989Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 4, commit: 2721, applied: 0, lastindex: 2721, lastterm: 4]"}
{"level":"warn","ts":"2023-08-05T01:29:47.991Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-08-05T01:29:48.014Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1715}
{"level":"info","ts":"2023-08-05T01:29:48.018Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":2273}
{"level":"info","ts":"2023-08-05T01:29:48.025Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-08-05T01:29:48.087Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-08-05T01:29:48.087Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-08-05T01:29:48.087Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2023-08-05T01:29:48.087Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-05T01:29:48.088Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-05T01:29:48.088Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-08-05T01:29:48.088Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2023-08-05T01:29:48.088Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-08-05T01:29:48.088Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-08-05T01:29:48.088Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-05T01:29:48.088Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-08-05T01:29:48.090Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-08-05T01:29:48.090Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-05T01:29:48.090Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-08-05T01:29:48.090Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-05T01:29:48.090Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-08-05T01:29:48.990Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2023-08-05T01:29:48.990Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2023-08-05T01:29:48.990Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2023-08-05T01:29:48.990Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2023-08-05T01:29:48.990Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2023-08-05T01:29:48.990Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2023-08-05T01:29:48.990Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2023-08-05T01:29:48.992Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-08-05T01:29:48.992Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-05T01:29:48.992Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-08-05T01:29:48.992Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-08-05T01:29:48.992Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-08-05T01:29:48.992Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-08-05T01:29:48.992Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-08-05T01:29:59.198Z","caller":"traceutil/trace.go:171","msg":"trace[1763092085] transaction","detail":"{read_only:false; response_revision:2338; number_of_response:1; }","duration":"234.268215ms","start":"2023-08-05T01:29:58.963Z","end":"2023-08-05T01:29:59.197Z","steps":["trace[1763092085] 'process raft request'  (duration: 234.163469ms)"],"step_count":1}
{"level":"info","ts":"2023-08-05T01:32:49.991Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-08-05T01:32:49.991Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
WARNING: 2023/08/05 01:32:49 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"info","ts":"2023-08-05T01:32:50.086Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-08-05T01:32:50.101Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-05T01:32:50.184Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-08-05T01:32:50.184Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  01:34:51 up 59 min,  0 users,  load average: 0.94, 0.91, 0.58
Linux minikube 5.10.16.3-microsoft-standard-WSL2 #1 SMP Fri Apr 2 22:23:49 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [74e16f5d708c] <==
* W0805 01:33:57.819616       1 genericapiserver.go:752] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0805 01:33:57.896320       1 handler.go:232] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0805 01:33:57.896370       1 genericapiserver.go:752] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0805 01:33:58.326715       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0805 01:33:58.326779       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0805 01:33:58.326941       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0805 01:33:58.327531       1 secure_serving.go:210] Serving securely on [::]:8443
I0805 01:33:58.327587       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0805 01:33:58.327732       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0805 01:33:58.327784       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0805 01:33:58.327807       1 available_controller.go:423] Starting AvailableConditionController
I0805 01:33:58.327819       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0805 01:33:58.327932       1 aggregator.go:150] waiting for initial CRD sync...
I0805 01:33:58.327990       1 controller.go:83] Starting OpenAPI AggregationController
I0805 01:33:58.328030       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0805 01:33:58.328075       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0805 01:33:58.328115       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0805 01:33:58.328162       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0805 01:33:58.328356       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0805 01:33:58.328394       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0805 01:33:58.328536       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I0805 01:33:58.328726       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0805 01:33:58.328935       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0805 01:33:58.329294       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0805 01:33:58.328129       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0805 01:33:58.328176       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0805 01:33:58.330828       1 controller.go:85] Starting OpenAPI controller
I0805 01:33:58.329685       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0805 01:33:58.331055       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0805 01:33:58.331209       1 controller.go:121] Starting legacy_token_tracking_controller
I0805 01:33:58.331243       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0805 01:33:58.331281       1 controller.go:85] Starting OpenAPI V3 controller
I0805 01:33:58.331292       1 naming_controller.go:291] Starting NamingConditionController
I0805 01:33:58.331302       1 establishing_controller.go:76] Starting EstablishingController
I0805 01:33:58.331329       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0805 01:33:58.331339       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0805 01:33:58.331422       1 crd_finalizer.go:266] Starting CRDFinalizer
I0805 01:33:58.485002       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0805 01:33:58.485061       1 aggregator.go:152] initial CRD sync complete...
I0805 01:33:58.485070       1 autoregister_controller.go:141] Starting autoregister controller
I0805 01:33:58.485077       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0805 01:33:58.490506       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0805 01:33:58.499433       1 shared_informer.go:318] Caches are synced for node_authorizer
I0805 01:33:58.528315       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0805 01:33:58.528350       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0805 01:33:58.528518       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0805 01:33:58.584980       1 shared_informer.go:318] Caches are synced for configmaps
I0805 01:33:58.585172       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0805 01:33:58.585217       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0805 01:33:58.585184       1 cache.go:39] Caches are synced for autoregister controller
I0805 01:33:59.140502       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0805 01:33:59.334275       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0805 01:34:00.494995       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0805 01:34:00.503100       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0805 01:34:00.588340       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0805 01:34:00.619282       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0805 01:34:00.627894       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0805 01:34:10.764866       1 controller.go:624] quota admission added evaluator for: endpoints
I0805 01:34:10.964796       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0805 01:34:10.964796       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io

* 
* ==> kube-apiserver [d4169f52777e] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0805 01:32:50.997579       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0805 01:32:50.997896       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0805 01:32:50.997991       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0805 01:32:50.998139       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0805 01:32:50.998161       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0805 01:32:50.999355       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0805 01:32:50.999413       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [84ef6d1381e1] <==
* I0805 01:34:10.685403       1 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0805 01:34:10.687384       1 controllermanager.go:638] "Started controller" controller="attachdetach"
I0805 01:34:10.687469       1 attach_detach_controller.go:343] "Starting attach detach controller"
I0805 01:34:10.687486       1 shared_informer.go:311] Waiting for caches to sync for attach detach
I0805 01:34:10.689344       1 controllermanager.go:638] "Started controller" controller="pvc-protection"
I0805 01:34:10.689378       1 pvc_protection_controller.go:102] "Starting PVC protection controller"
I0805 01:34:10.689386       1 shared_informer.go:311] Waiting for caches to sync for PVC protection
I0805 01:34:10.692625       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0805 01:34:10.699341       1 shared_informer.go:318] Caches are synced for TTL
I0805 01:34:10.699398       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0805 01:34:10.704973       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0805 01:34:10.705804       1 shared_informer.go:318] Caches are synced for persistent volume
I0805 01:34:10.710082       1 shared_informer.go:318] Caches are synced for endpoint
I0805 01:34:10.712503       1 shared_informer.go:318] Caches are synced for ReplicationController
I0805 01:34:10.713838       1 shared_informer.go:318] Caches are synced for job
I0805 01:34:10.722210       1 shared_informer.go:318] Caches are synced for node
I0805 01:34:10.722323       1 range_allocator.go:174] "Sending events to api server"
I0805 01:34:10.722434       1 range_allocator.go:178] "Starting range CIDR allocator"
I0805 01:34:10.722468       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0805 01:34:10.722477       1 shared_informer.go:318] Caches are synced for cidrallocator
I0805 01:34:10.724654       1 shared_informer.go:318] Caches are synced for taint
I0805 01:34:10.724737       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0805 01:34:10.724839       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0805 01:34:10.724903       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0805 01:34:10.724859       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0805 01:34:10.724921       1 taint_manager.go:211] "Sending events to api server"
I0805 01:34:10.724951       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0805 01:34:10.725991       1 shared_informer.go:318] Caches are synced for expand
I0805 01:34:10.726110       1 shared_informer.go:318] Caches are synced for HPA
I0805 01:34:10.728555       1 shared_informer.go:318] Caches are synced for crt configmap
I0805 01:34:10.728786       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0805 01:34:10.731043       1 shared_informer.go:318] Caches are synced for ephemeral
I0805 01:34:10.731076       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0805 01:34:10.733335       1 shared_informer.go:318] Caches are synced for GC
I0805 01:34:10.741032       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0805 01:34:10.746190       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0805 01:34:10.747679       1 shared_informer.go:318] Caches are synced for PV protection
I0805 01:34:10.777500       1 shared_informer.go:318] Caches are synced for disruption
I0805 01:34:10.779820       1 shared_informer.go:318] Caches are synced for cronjob
I0805 01:34:10.784677       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0805 01:34:10.788249       1 shared_informer.go:318] Caches are synced for attach detach
I0805 01:34:10.788300       1 shared_informer.go:318] Caches are synced for TTL after finished
I0805 01:34:10.789563       1 shared_informer.go:318] Caches are synced for PVC protection
I0805 01:34:10.796555       1 shared_informer.go:318] Caches are synced for deployment
I0805 01:34:10.872872       1 shared_informer.go:318] Caches are synced for namespace
I0805 01:34:10.884200       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0805 01:34:10.884478       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0805 01:34:10.885640       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0805 01:34:10.885674       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0805 01:34:10.886812       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0805 01:34:10.897359       1 shared_informer.go:318] Caches are synced for stateful set
I0805 01:34:10.936517       1 shared_informer.go:318] Caches are synced for service account
I0805 01:34:10.961673       1 shared_informer.go:318] Caches are synced for resource quota
I0805 01:34:10.993357       1 shared_informer.go:318] Caches are synced for resource quota
I0805 01:34:10.993424       1 shared_informer.go:318] Caches are synced for daemon sets
I0805 01:34:11.305678       1 shared_informer.go:318] Caches are synced for garbage collector
I0805 01:34:11.338690       1 shared_informer.go:318] Caches are synced for garbage collector
I0805 01:34:11.338733       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0805 01:34:38.196522       1 event.go:307] "Event occurred" object="default/hola-mundo-deployment-78475ff4cb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: hola-mundo-deployment-78475ff4cb-dlxxd"
I0805 01:34:44.271590       1 event.go:307] "Event occurred" object="default/hola-mundo-deployment-78475ff4cb" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: hola-mundo-deployment-78475ff4cb-nwnkn"

* 
* ==> kube-controller-manager [e809798c4c82] <==
* I0805 01:30:02.346885       1 controllermanager.go:638] "Started controller" controller="replicationcontroller"
I0805 01:30:02.347084       1 replica_set.go:201] "Starting controller" name="replicationcontroller"
I0805 01:30:02.347119       1 shared_informer.go:311] Waiting for caches to sync for ReplicationController
I0805 01:30:02.349308       1 controllermanager.go:638] "Started controller" controller="cronjob"
I0805 01:30:02.349427       1 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I0805 01:30:02.349436       1 shared_informer.go:311] Waiting for caches to sync for cronjob
I0805 01:30:02.351594       1 controllermanager.go:638] "Started controller" controller="csrapproving"
I0805 01:30:02.351676       1 certificate_controller.go:112] Starting certificate controller "csrapproving"
I0805 01:30:02.351684       1 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0805 01:30:02.354417       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0805 01:30:02.361198       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0805 01:30:02.384946       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0805 01:30:02.385052       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0805 01:30:02.385085       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0805 01:30:02.385290       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0805 01:30:02.389361       1 shared_informer.go:318] Caches are synced for disruption
I0805 01:30:02.392292       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0805 01:30:02.392731       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0805 01:30:02.392839       1 shared_informer.go:318] Caches are synced for taint
I0805 01:30:02.392886       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0805 01:30:02.392888       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0805 01:30:02.393025       1 taint_manager.go:211] "Sending events to api server"
I0805 01:30:02.393152       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0805 01:30:02.393242       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0805 01:30:02.393273       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0805 01:30:02.398243       1 shared_informer.go:318] Caches are synced for TTL after finished
I0805 01:30:02.400591       1 shared_informer.go:318] Caches are synced for HPA
I0805 01:30:02.406394       1 shared_informer.go:318] Caches are synced for endpoint
I0805 01:30:02.408542       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0805 01:30:02.411091       1 shared_informer.go:318] Caches are synced for service account
I0805 01:30:02.412336       1 shared_informer.go:318] Caches are synced for node
I0805 01:30:02.412400       1 range_allocator.go:174] "Sending events to api server"
I0805 01:30:02.412432       1 range_allocator.go:178] "Starting range CIDR allocator"
I0805 01:30:02.412439       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0805 01:30:02.412445       1 shared_informer.go:318] Caches are synced for cidrallocator
I0805 01:30:02.416894       1 shared_informer.go:318] Caches are synced for GC
I0805 01:30:02.418074       1 shared_informer.go:318] Caches are synced for ephemeral
I0805 01:30:02.419243       1 shared_informer.go:318] Caches are synced for job
I0805 01:30:02.419670       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0805 01:30:02.421551       1 shared_informer.go:318] Caches are synced for TTL
I0805 01:30:02.425405       1 shared_informer.go:318] Caches are synced for PVC protection
I0805 01:30:02.438274       1 shared_informer.go:318] Caches are synced for namespace
I0805 01:30:02.440567       1 shared_informer.go:318] Caches are synced for deployment
I0805 01:30:02.445144       1 shared_informer.go:318] Caches are synced for crt configmap
I0805 01:30:02.447309       1 shared_informer.go:318] Caches are synced for ReplicationController
I0805 01:30:02.449662       1 shared_informer.go:318] Caches are synced for cronjob
I0805 01:30:02.452013       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0805 01:30:02.488053       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0805 01:30:02.490565       1 shared_informer.go:318] Caches are synced for expand
I0805 01:30:02.503524       1 shared_informer.go:318] Caches are synced for PV protection
I0805 01:30:02.523354       1 shared_informer.go:318] Caches are synced for attach detach
I0805 01:30:02.542699       1 shared_informer.go:318] Caches are synced for persistent volume
I0805 01:30:02.555006       1 shared_informer.go:318] Caches are synced for resource quota
I0805 01:30:02.587407       1 shared_informer.go:318] Caches are synced for resource quota
I0805 01:30:02.595057       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0805 01:30:02.595689       1 shared_informer.go:318] Caches are synced for stateful set
I0805 01:30:02.598607       1 shared_informer.go:318] Caches are synced for daemon sets
I0805 01:30:02.993705       1 shared_informer.go:318] Caches are synced for garbage collector
I0805 01:30:02.993747       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0805 01:30:02.993718       1 shared_informer.go:318] Caches are synced for garbage collector

* 
* ==> kube-proxy [5c22cca55d95] <==
* I0805 01:34:00.285781       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0805 01:34:00.285862       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0805 01:34:00.285906       1 server_others.go:554] "Using iptables proxy"
I0805 01:34:01.136702       1 server_others.go:192] "Using iptables Proxier"
I0805 01:34:01.136753       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0805 01:34:01.136761       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0805 01:34:01.136812       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0805 01:34:01.136844       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0805 01:34:01.137549       1 server.go:658] "Version info" version="v1.27.3"
I0805 01:34:01.137589       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0805 01:34:01.138156       1 config.go:97] "Starting endpoint slice config controller"
I0805 01:34:01.138216       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0805 01:34:01.138291       1 config.go:315] "Starting node config controller"
I0805 01:34:01.138307       1 config.go:188] "Starting service config controller"
I0805 01:34:01.138318       1 shared_informer.go:311] Waiting for caches to sync for service config
I0805 01:34:01.138319       1 shared_informer.go:311] Waiting for caches to sync for node config
I0805 01:34:01.284866       1 shared_informer.go:318] Caches are synced for service config
I0805 01:34:01.284902       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0805 01:34:01.284925       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [826d65d4617c] <==
* E0805 01:29:47.897028       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I0805 01:29:50.200192       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0805 01:29:50.200257       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0805 01:29:50.200314       1 server_others.go:554] "Using iptables proxy"
I0805 01:29:50.302850       1 server_others.go:192] "Using iptables Proxier"
I0805 01:29:50.302950       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0805 01:29:50.302971       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0805 01:29:50.302997       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0805 01:29:50.303031       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0805 01:29:50.303663       1 server.go:658] "Version info" version="v1.27.3"
I0805 01:29:50.303711       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0805 01:29:50.304261       1 config.go:188] "Starting service config controller"
I0805 01:29:50.304297       1 shared_informer.go:311] Waiting for caches to sync for service config
I0805 01:29:50.304320       1 config.go:97] "Starting endpoint slice config controller"
I0805 01:29:50.304340       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0805 01:29:50.304440       1 config.go:315] "Starting node config controller"
I0805 01:29:50.304508       1 shared_informer.go:311] Waiting for caches to sync for node config
I0805 01:29:50.404380       1 shared_informer.go:318] Caches are synced for service config
I0805 01:29:50.404406       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0805 01:29:50.404693       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [0efbbdcbceb2] <==
* I0805 01:29:48.711455       1 serving.go:348] Generated self-signed cert in-memory
W0805 01:29:50.085647       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0805 01:29:50.085877       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0805 01:29:50.085975       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0805 01:29:50.086058       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0805 01:29:50.198809       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.3"
I0805 01:29:50.198845       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0805 01:29:50.200204       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0805 01:29:50.200317       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0805 01:29:50.200683       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0805 01:29:50.200767       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0805 01:29:50.300547       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0805 01:32:49.992888       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I0805 01:32:49.992904       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0805 01:32:49.993334       1 scheduling_queue.go:1135] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E0805 01:32:49.993396       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [418cab48a7f5] <==
* I0805 01:33:56.348766       1 serving.go:348] Generated self-signed cert in-memory
W0805 01:33:58.419710       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0805 01:33:58.419773       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0805 01:33:58.419785       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0805 01:33:58.419793       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0805 01:33:58.495812       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.3"
I0805 01:33:58.495866       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0805 01:33:58.496862       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0805 01:33:58.496923       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0805 01:33:58.497366       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0805 01:33:58.497400       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0805 01:33:58.597713       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Aug 05 01:34:02 minikube kubelet[1665]: E0805 01:34:02.984518    1665 kuberuntime_manager.go:1212] container &Container{Name:hola-mundo,Image:hola-mundo-k8s,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dhqdj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod hola-mundo-deployment-78475ff4cb-dfswz_default(3bbfb1ed-afd8-4845-a903-b5bc15e6050a): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 01:34:02 minikube kubelet[1665]: E0805 01:34:02.984597    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hola-mundo-deployment-78475ff4cb-dfswz" podUID=3bbfb1ed-afd8-4845-a903-b5bc15e6050a
Aug 05 01:34:03 minikube kubelet[1665]: E0805 01:34:03.422179    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ImagePullBackOff: \"Back-off pulling image \\\"hola-mundo-k8s\\\"\"" pod="default/hola-mundo-deployment-78475ff4cb-dfswz" podUID=3bbfb1ed-afd8-4845-a903-b5bc15e6050a
Aug 05 01:34:04 minikube kubelet[1665]: E0805 01:34:04.787187    1665 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:04 minikube kubelet[1665]: E0805 01:34:04.787244    1665 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:04 minikube kubelet[1665]: E0805 01:34:04.787360    1665 kuberuntime_manager.go:1212] container &Container{Name:hola-mundo,Image:hola-mundo-k8s,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vn55p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod hola-mundo-deployment-78475ff4cb-sh5cc_default(035d5a75-950f-4f70-9793-21489619fe69): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 01:34:04 minikube kubelet[1665]: E0805 01:34:04.787390    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hola-mundo-deployment-78475ff4cb-sh5cc" podUID=035d5a75-950f-4f70-9793-21489619fe69
Aug 05 01:34:05 minikube kubelet[1665]: E0805 01:34:05.103017    1665 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 01:34:05 minikube kubelet[1665]: E0805 01:34:05.103087    1665 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 05 01:34:05 minikube kubelet[1665]: E0805 01:34:05.434023    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ImagePullBackOff: \"Back-off pulling image \\\"hola-mundo-k8s\\\"\"" pod="default/hola-mundo-deployment-78475ff4cb-sh5cc" podUID=035d5a75-950f-4f70-9793-21489619fe69
Aug 05 01:34:10 minikube kubelet[1665]: I0805 01:34:10.471671    1665 scope.go:115] "RemoveContainer" containerID="5cf61b91cde7d5a6a6e54a9d65a6bd4ecbfeb3c67c3362d19158fdb2834462c5"
Aug 05 01:34:10 minikube kubelet[1665]: I0805 01:34:10.471891    1665 scope.go:115] "RemoveContainer" containerID="9a6f534d8270cb245cba4a395dc60e98aa3b8d93a60a647abd0f65b6f45587e6"
Aug 05 01:34:10 minikube kubelet[1665]: E0805 01:34:10.472124    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(53cdd0ad-d4e8-433f-92f1-618e7ce9b9e0)\"" pod="kube-system/storage-provisioner" podUID=53cdd0ad-d4e8-433f-92f1-618e7ce9b9e0
Aug 05 01:34:16 minikube kubelet[1665]: E0805 01:34:16.201292    1665 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 01:34:16 minikube kubelet[1665]: E0805 01:34:16.201375    1665 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 05 01:34:16 minikube kubelet[1665]: E0805 01:34:16.318854    1665 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:16 minikube kubelet[1665]: E0805 01:34:16.318929    1665 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:16 minikube kubelet[1665]: E0805 01:34:16.319221    1665 kuberuntime_manager.go:1212] container &Container{Name:hola-mundo,Image:hola-mundo-k8s,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dhqdj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod hola-mundo-deployment-78475ff4cb-dfswz_default(3bbfb1ed-afd8-4845-a903-b5bc15e6050a): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 01:34:16 minikube kubelet[1665]: E0805 01:34:16.319293    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hola-mundo-deployment-78475ff4cb-dfswz" podUID=3bbfb1ed-afd8-4845-a903-b5bc15e6050a
Aug 05 01:34:18 minikube kubelet[1665]: E0805 01:34:18.065062    1665 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:18 minikube kubelet[1665]: E0805 01:34:18.065103    1665 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:18 minikube kubelet[1665]: E0805 01:34:18.065212    1665 kuberuntime_manager.go:1212] container &Container{Name:hola-mundo,Image:hola-mundo-k8s,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vn55p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod hola-mundo-deployment-78475ff4cb-sh5cc_default(035d5a75-950f-4f70-9793-21489619fe69): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 01:34:18 minikube kubelet[1665]: E0805 01:34:18.065244    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hola-mundo-deployment-78475ff4cb-sh5cc" podUID=035d5a75-950f-4f70-9793-21489619fe69
Aug 05 01:34:24 minikube kubelet[1665]: I0805 01:34:24.313497    1665 scope.go:115] "RemoveContainer" containerID="9a6f534d8270cb245cba4a395dc60e98aa3b8d93a60a647abd0f65b6f45587e6"
Aug 05 01:34:27 minikube kubelet[1665]: E0805 01:34:27.248312    1665 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 01:34:27 minikube kubelet[1665]: E0805 01:34:27.248375    1665 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 05 01:34:28 minikube kubelet[1665]: E0805 01:34:28.317060    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ImagePullBackOff: \"Back-off pulling image \\\"hola-mundo-k8s\\\"\"" pod="default/hola-mundo-deployment-78475ff4cb-dfswz" podUID=3bbfb1ed-afd8-4845-a903-b5bc15e6050a
Aug 05 01:34:30 minikube kubelet[1665]: E0805 01:34:30.315993    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ImagePullBackOff: \"Back-off pulling image \\\"hola-mundo-k8s\\\"\"" pod="default/hola-mundo-deployment-78475ff4cb-sh5cc" podUID=035d5a75-950f-4f70-9793-21489619fe69
Aug 05 01:34:38 minikube kubelet[1665]: I0805 01:34:38.208381    1665 topology_manager.go:212] "Topology Admit Handler"
Aug 05 01:34:38 minikube kubelet[1665]: E0805 01:34:38.297864    1665 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 01:34:38 minikube kubelet[1665]: E0805 01:34:38.297923    1665 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available
Aug 05 01:34:38 minikube kubelet[1665]: I0805 01:34:38.309117    1665 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zxz4r\" (UniqueName: \"kubernetes.io/projected/64185e38-541f-4717-b8b9-fd07c48a9f92-kube-api-access-zxz4r\") pod \"hola-mundo-deployment-78475ff4cb-dlxxd\" (UID: \"64185e38-541f-4717-b8b9-fd07c48a9f92\") " pod="default/hola-mundo-deployment-78475ff4cb-dlxxd"
Aug 05 01:34:38 minikube kubelet[1665]: I0805 01:34:38.793313    1665 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="29f25b1bd5f83a78208126992325cca3232e4c65fc208215f50f171227210e4b"
Aug 05 01:34:38 minikube kubelet[1665]: I0805 01:34:38.812186    1665 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-dhqdj\" (UniqueName: \"kubernetes.io/projected/3bbfb1ed-afd8-4845-a903-b5bc15e6050a-kube-api-access-dhqdj\") pod \"3bbfb1ed-afd8-4845-a903-b5bc15e6050a\" (UID: \"3bbfb1ed-afd8-4845-a903-b5bc15e6050a\") "
Aug 05 01:34:38 minikube kubelet[1665]: I0805 01:34:38.813539    1665 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/3bbfb1ed-afd8-4845-a903-b5bc15e6050a-kube-api-access-dhqdj" (OuterVolumeSpecName: "kube-api-access-dhqdj") pod "3bbfb1ed-afd8-4845-a903-b5bc15e6050a" (UID: "3bbfb1ed-afd8-4845-a903-b5bc15e6050a"). InnerVolumeSpecName "kube-api-access-dhqdj". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 05 01:34:38 minikube kubelet[1665]: I0805 01:34:38.912945    1665 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-dhqdj\" (UniqueName: \"kubernetes.io/projected/3bbfb1ed-afd8-4845-a903-b5bc15e6050a-kube-api-access-dhqdj\") on node \"minikube\" DevicePath \"\""
Aug 05 01:34:40 minikube kubelet[1665]: I0805 01:34:40.318021    1665 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=3bbfb1ed-afd8-4845-a903-b5bc15e6050a path="/var/lib/kubelet/pods/3bbfb1ed-afd8-4845-a903-b5bc15e6050a/volumes"
Aug 05 01:34:41 minikube kubelet[1665]: E0805 01:34:41.063578    1665 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:41 minikube kubelet[1665]: E0805 01:34:41.063645    1665 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:41 minikube kubelet[1665]: E0805 01:34:41.063772    1665 kuberuntime_manager.go:1212] container &Container{Name:hola-mundo,Image:hola-mundo-k8s,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxz4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod hola-mundo-deployment-78475ff4cb-dlxxd_default(64185e38-541f-4717-b8b9-fd07c48a9f92): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 01:34:41 minikube kubelet[1665]: E0805 01:34:41.063830    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hola-mundo-deployment-78475ff4cb-dlxxd" podUID=64185e38-541f-4717-b8b9-fd07c48a9f92
Aug 05 01:34:41 minikube kubelet[1665]: E0805 01:34:41.826101    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ImagePullBackOff: \"Back-off pulling image \\\"hola-mundo-k8s\\\"\"" pod="default/hola-mundo-deployment-78475ff4cb-dlxxd" podUID=64185e38-541f-4717-b8b9-fd07c48a9f92
Aug 05 01:34:44 minikube kubelet[1665]: I0805 01:34:44.280667    1665 topology_manager.go:212] "Topology Admit Handler"
Aug 05 01:34:44 minikube kubelet[1665]: I0805 01:34:44.344883    1665 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gdjsx\" (UniqueName: \"kubernetes.io/projected/bc6b0938-8c2a-47f2-8f9d-14ea8469bcc7-kube-api-access-gdjsx\") pod \"hola-mundo-deployment-78475ff4cb-nwnkn\" (UID: \"bc6b0938-8c2a-47f2-8f9d-14ea8469bcc7\") " pod="default/hola-mundo-deployment-78475ff4cb-nwnkn"
Aug 05 01:34:44 minikube kubelet[1665]: E0805 01:34:44.918539    1665 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:44 minikube kubelet[1665]: E0805 01:34:44.918612    1665 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:44 minikube kubelet[1665]: E0805 01:34:44.918710    1665 kuberuntime_manager.go:1212] container &Container{Name:hola-mundo,Image:hola-mundo-k8s,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vn55p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod hola-mundo-deployment-78475ff4cb-sh5cc_default(035d5a75-950f-4f70-9793-21489619fe69): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 01:34:44 minikube kubelet[1665]: E0805 01:34:44.918744    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hola-mundo-deployment-78475ff4cb-sh5cc" podUID=035d5a75-950f-4f70-9793-21489619fe69
Aug 05 01:34:44 minikube kubelet[1665]: I0805 01:34:44.948816    1665 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4bd7e1b212191e6680410026c2532e031aa731f8f6fa4431d3e287225b294c07"
Aug 05 01:34:46 minikube kubelet[1665]: I0805 01:34:46.455563    1665 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-vn55p\" (UniqueName: \"kubernetes.io/projected/035d5a75-950f-4f70-9793-21489619fe69-kube-api-access-vn55p\") pod \"035d5a75-950f-4f70-9793-21489619fe69\" (UID: \"035d5a75-950f-4f70-9793-21489619fe69\") "
Aug 05 01:34:46 minikube kubelet[1665]: I0805 01:34:46.457664    1665 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/035d5a75-950f-4f70-9793-21489619fe69-kube-api-access-vn55p" (OuterVolumeSpecName: "kube-api-access-vn55p") pod "035d5a75-950f-4f70-9793-21489619fe69" (UID: "035d5a75-950f-4f70-9793-21489619fe69"). InnerVolumeSpecName "kube-api-access-vn55p". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 05 01:34:46 minikube kubelet[1665]: I0805 01:34:46.556080    1665 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-vn55p\" (UniqueName: \"kubernetes.io/projected/035d5a75-950f-4f70-9793-21489619fe69-kube-api-access-vn55p\") on node \"minikube\" DevicePath \"\""
Aug 05 01:34:46 minikube kubelet[1665]: E0805 01:34:46.766413    1665 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:46 minikube kubelet[1665]: E0805 01:34:46.766477    1665 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="hola-mundo-k8s:latest"
Aug 05 01:34:46 minikube kubelet[1665]: E0805 01:34:46.766631    1665 kuberuntime_manager.go:1212] container &Container{Name:hola-mundo,Image:hola-mundo-k8s,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdjsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod hola-mundo-deployment-78475ff4cb-nwnkn_default(bc6b0938-8c2a-47f2-8f9d-14ea8469bcc7): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Aug 05 01:34:46 minikube kubelet[1665]: E0805 01:34:46.766682    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for hola-mundo-k8s, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hola-mundo-deployment-78475ff4cb-nwnkn" podUID=bc6b0938-8c2a-47f2-8f9d-14ea8469bcc7
Aug 05 01:34:46 minikube kubelet[1665]: E0805 01:34:46.976352    1665 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hola-mundo\" with ImagePullBackOff: \"Back-off pulling image \\\"hola-mundo-k8s\\\"\"" pod="default/hola-mundo-deployment-78475ff4cb-nwnkn" podUID=bc6b0938-8c2a-47f2-8f9d-14ea8469bcc7
Aug 05 01:34:48 minikube kubelet[1665]: I0805 01:34:48.317639    1665 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=035d5a75-950f-4f70-9793-21489619fe69 path="/var/lib/kubelet/pods/035d5a75-950f-4f70-9793-21489619fe69/volumes"
Aug 05 01:34:49 minikube kubelet[1665]: E0805 01:34:49.344543    1665 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Aug 05 01:34:49 minikube kubelet[1665]: E0805 01:34:49.344597    1665 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal=allocatableMemory.available

* 
* ==> storage-provisioner [1f54660d924a] <==
* I0805 01:34:24.487566       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0805 01:34:24.495188       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0805 01:34:24.495257       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0805 01:34:42.167211       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0805 01:34:42.167639       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_c1502a70-354d-47e1-8a7d-2b847e587c5f!
I0805 01:34:42.167598       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"633041cd-6816-48e7-8d5d-d24c67a51c35", APIVersion:"v1", ResourceVersion:"2638", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_c1502a70-354d-47e1-8a7d-2b847e587c5f became leader
I0805 01:34:42.268654       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_c1502a70-354d-47e1-8a7d-2b847e587c5f!

* 
* ==> storage-provisioner [9a6f534d8270] <==
* I0805 01:34:00.200992       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0805 01:34:10.206863       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

